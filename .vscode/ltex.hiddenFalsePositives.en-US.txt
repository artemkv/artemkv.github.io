{"rule":"PHRASE_REPETITION","sentence":"^\\QMark particular points in time (when something was created, expiration date/time)\nMeasuring intervals of time (e.g. timeout)\nComputers have 2 types of physical clocks: time-of-day clocks and monotonic clocks\nTime-of-day clocks tell you what time it is, synchronized across machines over NTP, bad for measuring durations precisely (for things like leap seconds etc.), okeyish for timestamps\nMonotonic clocks only go forward, a counter (e.g. milliseconds since the machine was restarted), good for intervals and timestamps, can be used to time the code execution (\\E(?:Dummy|Ina|Jimmy-)[0-9]+\\Q is Java)\nIn distributed systems we also need logical clocks\nLogical clocks only provide the ordering of events\n\\E(?:Dummy|Ina|Jimmy-)[0-9]+\\Q: \"\\E(?:Dummy|Ina|Jimmy-)[0-9]+\\Q happened before \\E(?:Dummy|Ina|Jimmy-)[0-9]+\\Q\", important to determine potential causality\\E$"}
{"rule":"TOO_TO","sentence":"^\\QVector clocks guarantee that if \\E(?:Dummy|Ina|Jimmy-)[0-9]+\\Q, then \\E(?:Dummy|Ina|Jimmy-)[0-9]+\\Q and (the most importantly!) the implication goes in another direction too\nVector clocks are not only consistent with causality but they also characterize causality\nVector clock is a vector of integers, each component corresponding to a process\nVector clocks requires the number of processes being known and fixed upfront, as well as which component corresponds to which process\nRules:\nEvery process maintains a vector of integers, initialized to all zeroes; the length of a vector is a number of processes\nOn every event, a process will increment its own component in the vector\nWhen sending a message, a process first increments its own component in the vector and then sends its current vector together with the message\nWhen receiving a message, a process first increments its own component in the vector, then updates its vector clock to the max of its own vector clock and received vector clock\nMax is \"pointwise maximum\", e.g. max(1, 12, 4, 7, 0, 2) is 7, 12, 4\nComparing vectors is also component-wise\nVC(A) < VC(B) when VC(A)i <= VC(B)i for all i and VC(A) != VC(B)\nBasically, all the components of two vectors have to be less or equal, with at least one strictly smaller\nIf none of the two events is smaller than the other, they are considered to be concurrent (causally unrelated)\nSo this time, to find out whether 2 events are in \"happened before\" relationship, instead of thinking about graph reachability all you can do is to compare 2 vectors\\E$"}
{"rule":"TOO_TO","sentence":"^\\QVector clocks guarantee that if \\E(?:Dummy|Ina|Jimmy-)[0-9]+\\Q, then \\E(?:Dummy|Ina|Jimmy-)[0-9]+\\Q and (the most importantly!) the implication goes in another direction too\nVector clocks are not only consistent with causality, but they also characterize causality\nVector clock is a vector of integers, each component corresponding to a process\nVector clocks requires the number of processes being known and fixed upfront, as well as which component corresponds to which process\nRules:\nEvery process maintains a vector of integers, initialized to all zeroes; the length of a vector is a number of processes\nOn every event, a process will increment its own component in the vector\nWhen sending a message, a process first increments its own component in the vector and then sends its current vector together with the message\nWhen receiving a message, a process first increments its own component in the vector, then updates its vector clock to the max of its own vector clock and received vector clock\nMax is \"pointwise maximum\", e.g. max(1, 12, 4, 7, 0, 2) is 7, 12, 4\nComparing vectors is also component-wise\nVC(A) < VC(B) when VC(A)i <= VC(B)i for all i and VC(A) != VC(B)\nBasically, all the components of two vectors have to be less or equal, with at least one strictly smaller\nIf none of the two events is smaller than the other, they are considered to be concurrent (causally unrelated)\nSo this time, to find out whether 2 events are in \"happened before\" relationship, instead of thinking about graph reachability all you can do is to compare 2 vectors\\E$"}
{"rule":"A_INFINITIVE","sentence":"^\\QYou pick a particular replica to be Primary; all the other replicas are considered Backups\nClients only interact with the Primary replica, both for reads and writes\nWhen client makes a write request, the Primary broadcasts the write to all Backup replicas, waits for the acks, and only when every Backup replica has acknowledged the write (commit point), the Primary returns the response to the client\nWhen client makes a read request, the Primary simply returns the result to the client\nPrimary-backup replication is good for fault tolerance, but does not provide data locality or dividing up work\\E$"}
{"rule":"A_INFINITIVE","sentence":"^\\QWrites go to the first replica (Head replica)\nWhen client makes a write request, the Head replica sends the request to whatever replica is next, that one sends it to whatever replica is next etc. until we reach the last replica (Tail replica)\nThe tail replica acknowledges the write (commit point) and sends the reply to the client\nReads go directly to the tail replica\nThe order of replica have to be known to everyone\nChain replication is good for fault tolerance, but does not provide data locality.\\E$"}
{"rule":"CD_NN","sentence":"^\\Q(\"n choose k\")\nExample: taking random 5 cards from 52 card deck, there are 52 choose 5 possible hands\\E$"}
{"rule":"LARGE_NUMBER_OF","sentence":"^\\QOn average, every spin earns you $(1/61 + 1/22 + 1/4*4)\nThis is the expected value of a random variable, denoted as EX\nEX = Sum(x*P(X = x)) over all x\nOne interpretation: the average you get over a large number of experiments\nAnother, \"physical\", interpretation: the \"center of gravity\" of the PMF diagram\nOne of the most important properties of expectation is Linearity of expectation: EX+Y = EX + EY (true even if X and Y are dependent)\nIf you have a random variable X and a random variable Y = g(X), you can calculate EY as Sum(g(x)*Px(x)) over all x\nThis is called the law of the unconscious statistician (LOTUS)\nHarder way to calculate it: Sum(y*Py(y)) over all y\n!\\E$"}
{"rule":"ARROWS","sentence":"^\\QYou can do that, however, if you want to adhere to the autoregressive model, you need to make sure that, when you are making prediction, the pixels that you are using for prediction are consistent with the ordering you have chosen (don't look at the future pixels, but you can look at any of the past ones)\nThe way to achieve it is, again, by applying masking (to convolutions, meaning \\E(?:Dummy|Ina|Jimmy-)[0-9]+\\Q where X can be 1 or 0)\nThere is no reason why the order should be top-left -> bottom right\\E$"}
